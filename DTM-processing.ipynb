{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd99dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "print(tp.isa) # prints 'avx2', 'avx', 'sse2' or 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomotopy import DMRModel\n",
    "from tomotopy.utils import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca21c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import ldaseqmodel\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim import corpora, models, similarities\n",
    "from string import punctuation\n",
    "import numpy\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import signal\n",
    "import re\n",
    "import string\n",
    "import tomotopy as tp\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "from gensim.matutils import hellinger\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords - change stopword list - TODO\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def preprocessing(text):\n",
    "\tnegate_dict = {\n",
    "    \"couldn't\": \"could not\",\n",
    "\t\"can't\": \"can not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"hadn't\": \"had not\", \n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"wouldn't\": \"would not\"\n",
    "    }\n",
    "\tstemmer = PorterStemmer()\n",
    "\tstopwords_english = stopwords.words('english')\n",
    "\tstopwords_english.remove('not')\n",
    "\tstopwords_english.remove('no')\n",
    "\ttext = re.sub(r'$', '', str(text))\n",
    "\ttext = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', str(text))\n",
    "\ttext = re.sub(r'^RT[\\s]+', '', str(text))\n",
    "\ttext = re.sub(r'#', '', str(text))\n",
    "\ttext = re.sub(r'\\@\\w*', '', str(text))\n",
    "\ttext = re.sub(r'WHO', 'world health organization', str(text))\n",
    "\tfor negate_word in negate_dict.keys():\n",
    "\t\ttext = re.sub(negate_word, negate_dict[negate_word], str(text))\n",
    "\ttext = re.sub(r\"&\", ' and ', str(text))\n",
    "\ttext = text.replace('&amp', ' ')\n",
    "\ttext = text.replace('amp', ' ')\n",
    "\ttext = re.sub(r\"[^0-9a-zA-Z]+\", ' ', str(text))\n",
    "\ttokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\ttext_tokens = tokenizer.tokenize(text)\n",
    "\tclean_text = []\n",
    "\tfor word in text_tokens:\n",
    "\t\tif (word not in stopwords_english and word not in string.punctuation):\n",
    "\t\t\tstem_word = stemmer.stem(word)\n",
    "\t\t\tclean_text.append(stem_word)\n",
    "\treturn ' '.join(clean_text)\n",
    "\n",
    "def preprocessing_to_token(text):\n",
    "\tnegate_dict = {\n",
    "    \"couldn't\": \"could not\",\n",
    "\t\"can't\": \"can not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"hadn't\": \"had not\", \n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"wouldn't\": \"would not\"\n",
    "    }\n",
    "\tstemmer = PorterStemmer()\n",
    "\tstopwords_english = stopwords.words('english')\n",
    "\tstopwords_english.remove('not')\n",
    "\tstopwords_english.remove('no')\n",
    "\ttext = re.sub(r'$', '', str(text))\n",
    "\ttext = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', str(text))\n",
    "\ttext = re.sub(r'^RT[\\s]+', '', str(text))\n",
    "\ttext = re.sub(r'#', '', str(text))\n",
    "\ttext = re.sub(r'\\@\\w*', '', str(text))\n",
    "\ttext = re.sub(r'WHO', 'world health organization', str(text))\n",
    "\tfor negate_word in negate_dict.keys():\n",
    "\t\ttext = re.sub(negate_word, negate_dict[negate_word], str(text))\n",
    "\ttext = re.sub(r\"&\", ' and ', str(text))\n",
    "\ttext = text.replace('&amp', ' ')\n",
    "\ttext = text.replace('amp', ' ')\n",
    "\ttext = re.sub(r\"[^0-9a-zA-Z]+\", ' ', str(text))\n",
    "\ttokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\ttext_tokens = tokenizer.tokenize(text)\n",
    "\tclean_text = []\n",
    "\tfor word in text_tokens:\n",
    "\t\tif (word not in stopwords_english and word not in string.punctuation):\n",
    "\t\t\tstem_word = stemmer.stem(word)\n",
    "\t\t\tclean_text.append(stem_word)\n",
    "\treturn clean_text\n",
    "def converttimestamp(timestamp_string):\n",
    "    obj = datetime.datetime.strptime(timestamp_string, '%Y-%m-%d')\n",
    "    return obj\n",
    "\n",
    "def calc_timedelta(ts1):\n",
    "    ts1_obj = converttimestamp(ts1)\n",
    "    ts2_obj = converttimestamp(\"2020-02-01\")\n",
    "    return (ts1_obj - ts2_obj).days\n",
    "def calc_timestamp_withdelta(delta, delta_0):\n",
    "    ts1_obj = converttimestamp(delta_0)\n",
    "    timedelta_obj = datetime.timedelta(days = delta)\n",
    "    ts2_obj = ts1_obj + timedelta_obj\n",
    "    return ts2_obj.strftime(\"%Y-%m-%d\")\n",
    "def splitdict(string):\n",
    "    i = 0\n",
    "    dict_list = []\n",
    "    for index in range(len(string) - 2):\n",
    "        if string[index] == \"}\" and string[index+1] == \"{\" and string[index+2].isalpha() == False:\n",
    "            sub_string = string[i:index+1]\n",
    "            i = index + 1\n",
    "            try:\n",
    "                dict1 = json.loads(sub_string)\n",
    "                dict_list.append(dict1)\n",
    "            except Exception as e:\n",
    "                dict1 = []\n",
    "                #print(sub_string)\n",
    "                print(\"ERROR!\")\n",
    "                print(e)\n",
    "                pass\n",
    "    sub_string = string[i:]\n",
    "    dict2 = json.loads(sub_string)\n",
    "    dict_list.append(dict2)\n",
    "    return dict_list\n",
    "def corpus_from_keyword_timeslice_tomoto(keyword):\n",
    "    root_path = \"\"\n",
    "    keyword_path = os.path.join(root_path, keyword)\n",
    "    corpus = []\n",
    "    time_slice = []\n",
    "    distinct_dict = {}\n",
    "    delta_0 = \"2019-10-01\"\n",
    "    for i in range(0, 177):\n",
    "        distinct_dict = {}\n",
    "        # From day 0 to day 178\n",
    "        dayfolder = calc_timestamp_withdelta(i, \"2019-10-01\")\n",
    "        dayfolder_path = os.path.join(keyword_path, dayfolder)\n",
    "        for file in os.listdir(dayfolder_path):\n",
    "            with open(os.path.join(dayfolder_path, file), 'r', encoding = 'utf-8') as f:\n",
    "                try:\n",
    "                    listdict = splitdict(f.read())\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    listdict = []\n",
    "                #DO WORK in 180 seconds for 100 tweets.\n",
    "                try:\n",
    "                    for each in listdict:\n",
    "                        if \"retweeted_status\" in each.keys():\n",
    "                            st = preprocessing(each['retweeted_status'][\"full_text\"])\n",
    "                        else:\n",
    "                            st = preprocessing(each[\"full_text\"])\n",
    "                        if st in distinct_dict:\n",
    "                            distinct_dict[st] += 1\n",
    "                        else:\n",
    "                            distinct_dict[st] = 1\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "        for each in distinct_dict:\n",
    "            yield each, None, {'timepoint':i}\n",
    "        print(\"Finished day \" + str(i) + \" ...with length \" +str(len(distinct_dict)))\n",
    "def corpus_separation(keyword, mdl_done):\n",
    "    root_path = \"\"\n",
    "    out_path = \"\"\n",
    "    keyword_path = os.path.join(root_path, keyword)\n",
    "    keyword_out_path = os.path.join(out_path, keyword)\n",
    "    os.makedirs(keyword_out_path, exist_ok = True)\n",
    "    topic_num = len(mdl.get_count_by_topics()[0])\n",
    "    corpus = []\n",
    "    time_slice = []\n",
    "    distinct_dict = {}\n",
    "    json_dict = {}\n",
    "    json_dict_count = {}\n",
    "    json_dict_output_num = {}\n",
    "    for i in range(0, topic_num):\n",
    "        json_dict_count[i] = 0\n",
    "        json_dict[i] = []\n",
    "        json_dict_output_num[i] = 0\n",
    "\n",
    "    delta_0 = \"2019-10-01\"\n",
    "    signal_a = False\n",
    "           \n",
    "    for i in range(0, 177):\n",
    "        distinct_dict = {}\n",
    "        # From day 0 to day 178\n",
    "        dayfolder = calc_timestamp_withdelta(i, \"2019-10-01\")\n",
    "        dayfolder_path = os.path.join(keyword_path, dayfolder)\n",
    "        out_dayfolder_path = os.path.join(keyword_out_path, dayfolder)\n",
    "        for file in os.listdir(dayfolder_path):\n",
    "            print(file)\n",
    "            with open(os.path.join(dayfolder_path, file), 'r', encoding = 'utf-8') as f:\n",
    "                try:\n",
    "                    listdict = splitdict(f.read())\n",
    "                    #print(listdict)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    listdict = []\n",
    "                #DO WORK in 180 seconds for 100 tweets.\n",
    "                for each in listdict:\n",
    "                    try:\n",
    "                        if \"retweeted_status\" in each.keys():\n",
    "                            st = preprocessing(each['retweeted_status'][\"full_text\"])\n",
    "                            singal_a = False\n",
    "                        else:\n",
    "                            st = preprocessing(each[\"full_text\"])\n",
    "                            singal_a = True\n",
    "                        if st in distinct_dict:\n",
    "                            distinct_dict[st] += 1\n",
    "                        else:\n",
    "                            distinct_dict[st] = 1\n",
    "                            if singal_a:\n",
    "                                doc_inst = mdl_done.make_doc(preprocessing_to_token(each[\"full_text\"]), timepoint = i)\n",
    "                            else:\n",
    "                                doc_inst = mdl_done.make_doc(preprocessing_to_token(each['retweeted_status'][\"full_text\"]), timepoint = i)\n",
    "                            topic_dist, ll = mdl_done.infer(doc_inst, iter = 1, parallel = 1)\n",
    "                            #print(topic_dist)\n",
    "                            topic_distribution_list = list(topic_dist)\n",
    "                            for x in range(0, topic_num):\n",
    "                                if topic_distribution_list[x] > 0.12:\n",
    "                                    json_dict_count[x] += 1\n",
    "                                    json_dict[x].append(each)\n",
    "                                    if json_dict_count[x] == 100:\n",
    "                                        json_dict_output_num[x] += 1\n",
    "                                        outfile = str(json_dict_output_num[x]) + \".json\"\n",
    "                                        outfile_path = os.path.join(out_dayfolder_path, str(x))\n",
    "                                        os.makedirs(outfile_path, exist_ok = True)\n",
    "                                        outfile_path = os.path.join(outfile_path, outfile)\n",
    "                                        with open(outfile_path, 'w', encoding = \"utf-8\") as w:\n",
    "                                            while(len(json_dict[x]) > 0):\n",
    "                                                content = json_dict[x].pop()\n",
    "                                                w.write(json.dumps(content))\n",
    "                                            w.close()\n",
    "                                        json_dict_count[x] = 0\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        continue\n",
    "        print(\"Finished day \" + str(i))\n",
    "        for x in range(0, topic_num):\n",
    "            outfile_path = os.path.join(out_dayfolder_path, str(x))\n",
    "            os.makedirs(outfile_path, exist_ok = True)\n",
    "            if json_dict_count[x] != 0:\n",
    "                json_dict_output_num[x] += 1\n",
    "                outfile = str(json_dict_output_num[x]) + \".json\"\n",
    "                final_path = os.path.join(outfile_path, outfile)\n",
    "                with open(final_path, 'w', encoding = \"utf-8\") as w:\n",
    "                    while(len(json_dict[x]) > 0):\n",
    "                        content = json_dict[x].pop()\n",
    "                        w.write(json.dumps(content))\n",
    "                    w.close()\n",
    "                json_dict_count[x] = 0\n",
    "                json_dict_output_num[x] = 0\n",
    "                \n",
    "def corpus_separation_day(keyword, mdl_done, day):\n",
    "    root_path = \"\"\n",
    "    out_path = \"\"\n",
    "    keyword_path = os.path.join(root_path, keyword)\n",
    "    keyword_out_path = os.path.join(out_path, keyword)\n",
    "    os.makedirs(keyword_out_path, exist_ok = True)\n",
    "    topic_num = len(mdl.get_count_by_topics()[0])\n",
    "    corpus = []\n",
    "    time_slice = []\n",
    "    distinct_dict = {}\n",
    "    json_dict = {}\n",
    "    json_dict_count = {}\n",
    "    json_dict_output_num = {}\n",
    "    for i in range(0, topic_num):\n",
    "        json_dict_count[i] = 0\n",
    "        json_dict[i] = []\n",
    "        json_dict_output_num[i] = 0\n",
    "\n",
    "    delta_0 = \"2019-10-01\"\n",
    "    signal_a = False\n",
    "    distinct_dict = {}\n",
    "    # From day 0 to day 178\n",
    "    dayfolder = calc_timestamp_withdelta(day, \"2019-10-01\")\n",
    "    dayfolder_path = os.path.join(keyword_path, dayfolder)\n",
    "    out_dayfolder_path = os.path.join(keyword_out_path, dayfolder)\n",
    "    for file in os.listdir(dayfolder_path):\n",
    "        print(file)\n",
    "        with open(os.path.join(dayfolder_path, file), 'r', encoding = 'utf-8') as f:\n",
    "            try:\n",
    "                listdict = splitdict(f.read())\n",
    "                #print(listdict)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                listdict = []\n",
    "            #DO WORK in 180 seconds for 100 tweets.\n",
    "            for each in listdict:\n",
    "                try:\n",
    "                    if \"retweeted_status\" in each.keys():\n",
    "                        st = preprocessing(each['retweeted_status'][\"full_text\"])\n",
    "                        singal_a = False\n",
    "                    else:\n",
    "                        st = preprocessing(each[\"full_text\"])\n",
    "                        singal_a = True\n",
    "                    if st in distinct_dict:\n",
    "                        distinct_dict[st] += 1\n",
    "                        \n",
    "                    else:\n",
    "                        distinct_dict[st] = 1\n",
    "                        if singal_a:\n",
    "                            doc_inst = mdl_done.make_doc(preprocessing_to_token(each[\"full_text\"]), timepoint = day)\n",
    "                        else:\n",
    "                            doc_inst = mdl_done.make_doc(preprocessing_to_token(each['retweeted_status'][\"full_text\"]), timepoint = day)\n",
    "                        topic_dist, ll = mdl_done.infer(doc_inst, iter = 1, parallel = 1)\n",
    "                        #print(topic_dist)\n",
    "                        topic_distribution_list = list(topic_dist)\n",
    "                        for x in range(0, topic_num):\n",
    "                            if topic_distribution_list[x] > 0.12:\n",
    "                                json_dict_count[x] += 1\n",
    "                                json_dict[x].append(each)\n",
    "                                if json_dict_count[x] == 100:\n",
    "                                    json_dict_output_num[x] += 1\n",
    "                                    outfile = str(json_dict_output_num[x]) + \".json\"\n",
    "                                    outfile_path = os.path.join(out_dayfolder_path, str(x))\n",
    "                                    os.makedirs(outfile_path, exist_ok = True)\n",
    "                                    outfile_path = os.path.join(outfile_path, outfile)\n",
    "                                    with open(outfile_path, 'w', encoding = \"utf-8\") as w:\n",
    "                                        while(len(json_dict[x]) > 0):\n",
    "                                            content = json_dict[x].pop()\n",
    "                                            w.write(json.dumps(content))\n",
    "                                        w.close()\n",
    "                                    json_dict_count[x] = 0\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "    print(\"Finished day \" + str(day))\n",
    "    for x in range(0, topic_num):\n",
    "        outfile_path = os.path.join(out_dayfolder_path, str(x))\n",
    "        os.makedirs(outfile_path, exist_ok = True)\n",
    "        if json_dict_count[x] != 0:\n",
    "            json_dict_output_num[x] += 1\n",
    "            outfile = str(json_dict_output_num[x]) + \".json\"\n",
    "            final_path = os.path.join(outfile_path, outfile)\n",
    "            with open(final_path, 'w', encoding = \"utf-8\") as w:\n",
    "                while(len(json_dict[x]) > 0):\n",
    "                    content = json_dict[x].pop()\n",
    "                    w.write(json.dumps(content))\n",
    "                w.close()\n",
    "            json_dict_count[x] = 0\n",
    "            json_dict_output_num[x] = 0\n",
    "def corpus_check_stat(keyword, day):\n",
    "    return 0\n",
    "def corpus_separation_day_new(keyword, mdl_done, day):\n",
    "    root_path = \"\"\n",
    "    out_path = \"\"\n",
    "    keyword_path = os.path.join(root_path, keyword)\n",
    "    keyword_out_path = os.path.join(out_path, keyword)\n",
    "    os.makedirs(keyword_out_path, exist_ok = True)\n",
    "    topic_num = len(mdl.get_count_by_topics()[0])\n",
    "    corpus = []\n",
    "    time_slice = []\n",
    "    distinct_dict = {}\n",
    "    json_dict = {}\n",
    "    json_dict_count = {}\n",
    "    json_dict_output_num = {}\n",
    "    for i in range(0, topic_num):\n",
    "        json_dict_count[i] = 0\n",
    "        json_dict[i] = []\n",
    "        json_dict_output_num[i] = 0\n",
    "\n",
    "    delta_0 = \"2019-10-01\"\n",
    "    signal_a = False\n",
    "    distinct_dict = {}\n",
    "    # From day 0 to day 178\n",
    "    dayfolder = calc_timestamp_withdelta(day, \"2019-10-01\")\n",
    "    dayfolder_path = os.path.join(keyword_path, dayfolder)\n",
    "    out_dayfolder_path = os.path.join(keyword_out_path, dayfolder)\n",
    "    count = 0\n",
    "    for file in os.listdir(dayfolder_path):\n",
    "        print(file)\n",
    "        with open(os.path.join(dayfolder_path, file), 'r', encoding = 'utf-8') as f:\n",
    "            try:\n",
    "                listdict = splitdict(f.read())\n",
    "                #print(listdict)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                listdict = []\n",
    "            #DO WORK in 180 seconds for 100 tweets.\n",
    "            for each in listdict:\n",
    "                try:\n",
    "                    if \"retweeted_status\" in each.keys():\n",
    "                        st = preprocessing(each['retweeted_status'][\"full_text\"])\n",
    "                        singal_a = False\n",
    "                    else:\n",
    "                        st = preprocessing(each[\"full_text\"])\n",
    "                        singal_a = True\n",
    "                    if st in distinct_dict:\n",
    "                        #distinct_dict[st] += 1\n",
    "                        #if already in, we read the distinct_dict[st] which is a list of numbers storing the distribution_list of topic.\n",
    "                        topic_distribution_list = distinct_dict[st]\n",
    "                    else:\n",
    "                        #distinct_dict[st] = 1\n",
    "                        if singal_a:\n",
    "                            doc_inst = mdl_done.make_doc(preprocessing_to_token(each[\"full_text\"]), timepoint = day)\n",
    "                        else:\n",
    "                            doc_inst = mdl_done.make_doc(preprocessing_to_token(each['retweeted_status'][\"full_text\"]), timepoint = day)\n",
    "                        topic_dist, ll = mdl_done.infer(doc_inst, iter = 1, parallel = 1)\n",
    "                        #print(topic_dist)\n",
    "                        topic_distribution_list = list(topic_dist)\n",
    "                        distinct_dict[st] = topic_distribution_list                    \n",
    "                    for x in range(0, topic_num):\n",
    "                        if topic_distribution_list[x] > 0.12:\n",
    "                            json_dict_count[x] += 1\n",
    "                            count += 1\n",
    "                            json_dict[x].append(each)\n",
    "                            if json_dict_count[x] == 100:\n",
    "                                json_dict_output_num[x] += 1\n",
    "                                outfile = str(json_dict_output_num[x]) + \".json\"\n",
    "                                outfile_path = os.path.join(out_dayfolder_path, str(x))\n",
    "                                os.makedirs(outfile_path, exist_ok = True)\n",
    "                                outfile_path = os.path.join(outfile_path, outfile)\n",
    "                                with open(outfile_path, 'w', encoding = \"utf-8\") as w:\n",
    "                                    while(len(json_dict[x]) > 0):\n",
    "                                        content = json_dict[x].pop()\n",
    "                                        w.write(json.dumps(content))\n",
    "                                    w.close()\n",
    "                                json_dict_count[x] = 0\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "    print(\"Finished day \" + str(day) + \" ... with count \" + str(count))\n",
    "    for x in range(0, topic_num):\n",
    "        outfile_path = os.path.join(out_dayfolder_path, str(x))\n",
    "        os.makedirs(outfile_path, exist_ok = True)\n",
    "        if json_dict_count[x] != 0:\n",
    "            json_dict_output_num[x] += 1\n",
    "            outfile = str(json_dict_output_num[x]) + \".json\"\n",
    "            final_path = os.path.join(outfile_path, outfile)\n",
    "            with open(final_path, 'w', encoding = \"utf-8\") as w:\n",
    "                while(len(json_dict[x]) > 0):\n",
    "                    content = json_dict[x].pop()\n",
    "                    w.write(json.dumps(content))\n",
    "                w.close()\n",
    "            json_dict_count[x] = 0\n",
    "            json_dict_output_num[x] = 0\n",
    "def chunking(lst, n):\n",
    "    '''\n",
    "    Given lst,\n",
    "    return a list of object in lst with max length of n\n",
    "    '''\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54188e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keyword = \"africa\"\n",
    "porter_stemmer = nltk.PorterStemmer().stem\n",
    "corpus2 = tp.utils.Corpus(\n",
    "    tokenizer=tp.utils.SimpleTokenizer(porter_stemmer)\n",
    ")\n",
    "corpus2.process(corpus_from_keyword_timeslice_tomoto(keyword))\n",
    "\n",
    "mdl = tp.DTModel(min_cf=100,min_df = 70, k=25, t=177, corpus=corpus2)\n",
    "mdl.train(0)\n",
    "\n",
    "print('Num docs:{}, Num Vocabs:{}, Total Words:{}'.format(\n",
    "    len(mdl.docs), len(mdl.used_vocabs), mdl.num_words\n",
    "))\n",
    "print('Removed Top words: ', *mdl.removed_top_words)\n",
    "\n",
    "# Let's train the model\n",
    "for i in range(0, 1000, 20):\n",
    "    print('Iteration: {:04}, LL per word: {:.4}'.format(i, mdl.ll_per_word))\n",
    "    mdl.train(20)\n",
    "print('Iteration: {:04}, LL per word: {:.4}'.format(1000, mdl.ll_per_word))\n",
    "\n",
    "mdl.summary()\n",
    "mdl.save(\"DTM-mdl-\" + keyword + \".save\")\n",
    "topic_dist_by_time = np.zeros(shape=[mdl.num_timepoints, mdl.k], dtype=np.float)\n",
    "for doc in mdl.docs:\n",
    "    topic_dist_by_time[doc.timepoint] += doc.get_topic_dist()\n",
    "\n",
    "topic_dist_by_time /= mdl.num_docs_by_timepoint[:, np.newaxis]\n",
    "\n",
    "for k in range(mdl.k):\n",
    "    print('Topic #{}'.format(k), *(w for w, _ in mdl.get_topic_words(k, 0, top_n=5)))\n",
    "    print(topic_dist_by_time[:, k])\n",
    "\n",
    "for timepoint in range(mdl.num_timepoints):\n",
    "    topic_term_dists = np.stack([mdl.get_topic_word_dist(k, timepoint=timepoint) for k in range(mdl.k)])\n",
    "    doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs if doc.timepoint == timepoint])\n",
    "    doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "    doc_lengths = np.array([len(doc.words) for doc in mdl.docs if doc.timepoint == timepoint])\n",
    "    vocab = list(mdl.used_vocabs)\n",
    "    term_frequency = mdl.used_vocab_freq\n",
    "\n",
    "    prepared_data = pyLDAvis.prepare(\n",
    "        topic_term_dists, \n",
    "        doc_topic_dists, \n",
    "        doc_lengths, \n",
    "        vocab, \n",
    "        term_frequency,\n",
    "        start_index=0,\n",
    "        sort_topics=False\n",
    "    )\n",
    "    #pyLDAvis.save_html(prepared_data, 'dtmvis_outbreak_{}.html'.format(timepoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c6515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "process_list = []\n",
    "for each in range(0, 177):\n",
    "    st = multiprocessing.Process(target = corpus_separation_day_new, args = (keyword, mdl, each))\n",
    "    process_list.append(st)\n",
    "for each in chunking(process_list, 30):\n",
    "    for each_process in each:\n",
    "        each_process.start()\n",
    "    for each_process in each:\n",
    "        each_process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d0587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
