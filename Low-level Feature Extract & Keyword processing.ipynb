{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de50b31",
   "metadata": {},
   "source": [
    "Extract keyword embeddings from DTModel, create a unique word list, then record the top-10 keyword for all topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Import bert, bert-base-uncased\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "def calc_timedelta(ts1):\n",
    "    ts1_obj = ts1\n",
    "    ts2_obj = datetime.datetime.strptime(\"2019-10-01\", '%Y-%m-%d')\n",
    "    return (ts1_obj - ts2_obj).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTM model path\n",
    "DTM_Model_Path = \"/\"\n",
    "output_path = \"/\"\n",
    "#DTM Model name: DTM-mdl-<keyword>.save\n",
    "import tomotopy as tp\n",
    "import os\n",
    "import csv\n",
    "mdl = tp.DTModel()\n",
    "topic_num = 25\n",
    "timepoints_num = 177\n",
    "unique_keyword_dict = {} # dict of unique keywords, in order to minimize cpu cost\n",
    "\n",
    "for file in os.listdir(DTM_Model_Path):\n",
    "    if file.startswith(\"DTM-mdl-\"):\n",
    "        print(file)\n",
    "        keyword = file.split(\"-\")[2].split(\".\")[0]\n",
    "        output_list = []\n",
    "        mdl = tp.DTModel()\n",
    "        path = os.path.join(DTM_Model_Path, file)\n",
    "        mdl = mdl.load(file)\n",
    "        #Next we extract the features from the model.\n",
    "        for topic in range(0, topic_num):\n",
    "            for timepoint in range(0, timepoints_num):\n",
    "                input_text_dict = mdl.get_topic_words(topic_id = topic, timepoint = timepoint, top_n = 10) #We set it to get 10 keywords.\n",
    "                for each in input_text_dict:\n",
    "                    if each[0] in unique_keyword_dict:\n",
    "                        out_list = [topic, timepoint, each[0], each[1], unique_keyword_dict[each[0]]]\n",
    "                    else:\n",
    "                        inputs = tokenizer(each[0], return_tensors = \"pt\").to(device)\n",
    "                        outputs = model(**inputs)\n",
    "                        unique_keyword_dict[each[0]] = outputs.pooler_output.tolist()\n",
    "                        out_list = [topic, timepoint, each[0], each[1], outputs.pooler_output.tolist()]\n",
    "                    output_list.append(out_list)\n",
    "        name = \"DTM-BERT-\" + keyword + \".csv\"\n",
    "        filename3 = os.path.join(output_path, name)\n",
    "        with open(filename3, 'w') as csvfile: \n",
    "            # creating a csv writer object \n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow([\"topic_num\", \"date_num\", \"keyword_string\", \"keyword_freq\", \"keyword_embedding\"])\n",
    "            csvwriter.writerows(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_keyword_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.get_topic_word_dist(topic_id = 15, timepoint = 155).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150db108",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we already have a unique_keyword_dict, this could go much faster.\n",
    "output_path = \"/"\n",
    "for file in os.listdir(DTM_Model_Path):\n",
    "    if file.startswith(\"DTM-mdl-\"):\n",
    "        print(file)\n",
    "        keyword = file.split(\"-\")[2].split(\".\")[0]\n",
    "        output_list = []\n",
    "        mdl = tp.DTModel()\n",
    "        path = os.path.join(DTM_Model_Path, file)\n",
    "        mdl = mdl.load(file)\n",
    "        #Next we extract the features from the model.\n",
    "        #each file is in the following format: topic, timepoint, freq_list, vector_list\n",
    "        for topic in range(0, topic_num):\n",
    "            for timepoint in range(0, timepoints_num):\n",
    "                keyword_freq = mdl.get_topic_word_dist(topic_id = topic, timepoint = timepoint).tolist()\n",
    "                #the list start with the biggest number, conviniently.\n",
    "                sum_x = 0\n",
    "                feature_list_freq = [0,0,0,0,0,0]\n",
    "                for i in range(0, len(keyword_freq)):\n",
    "                    sum_x += keyword_freq[i]\n",
    "                    if sum_x >= 0.25 and feature_list_freq[0] == 0:\n",
    "                        feature_list_freq[0] = i + 1\n",
    "                    if sum_x >= 0.40 and feature_list_freq[1] == 0:\n",
    "                        feature_list_freq[1] = i + 1\n",
    "                    if sum_x >= 0.50 and feature_list_freq[2] == 0:\n",
    "                        feature_list_freq[2] = i + 1\n",
    "                    if sum_x >= 0.65 and feature_list_freq[3] == 0:\n",
    "                        feature_list_freq[3] = i + 1\n",
    "                    if sum_x >= 0.75 and feature_list_freq[4] == 0:\n",
    "                        feature_list_freq[4] = i + 1\n",
    "                    if sum_x >= 0.95 and feature_list_freq[5] == 0:\n",
    "                        feature_list_freq[5] = i + 1\n",
    "                for i in range(0, len(feature_list_freq)):\n",
    "                    if feature_list_freq[i] == 0:\n",
    "                        feature_list_freq[i] = len(keyword_freq)\n",
    "                #Count the overall vector:\n",
    "                input_text_dict = mdl.get_topic_words(topic_id = topic, timepoint = timepoint, top_n = 10)\n",
    "                res_list = [] #vector of the current topic timepoint\n",
    "                for each in input_text_dict:\n",
    "                    if each[0] in unique_keyword_dict:\n",
    "                        embedding_list = unique_keyword_dict[each[0]][0].copy()\n",
    "                    else:\n",
    "                        inputs = tokenizer(each[0], return_tensors = \"pt\").to(device)\n",
    "                        outputs = model(**inputs)\n",
    "                        unique_keyword_dict[each[0]] = outputs.pooler_output.tolist()                                     \n",
    "                        embedding_list = unique_keyword_dict[each[0]][0].copy()\n",
    "                    #print(embedding_list)\n",
    "                    for i in range(0, len(embedding_list)):\n",
    "                        embedding_list[i] = embedding_list[i] * each[1]\n",
    "                    if len(res_list) == 0:\n",
    "                        res_list = embedding_list.copy()                                         \n",
    "                    else:                       \n",
    "                        origin = res_list.copy()\n",
    "                        res_list = []\n",
    "                        for i in range(0, len(origin)):\n",
    "                            res_list.append(origin[i] + embedding_list[i])\n",
    "                output_list.append([topic, timepoint, feature_list_freq, res_list])\n",
    "        filename3 = \"DTM-Highlevel-\" + keyword + \".csv\"\n",
    "        file_path = os.path.join(output_path, filename3)\n",
    "        with open(file_path, 'w') as csvfile: \n",
    "            # creating a csv writer object \n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow([\"topic_num\", \"date_num\", \"frequency_list\", \"topic_vector_list\"])\n",
    "            csvwriter.writerows(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd018d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now that we have the high-level topic/ cluster feature, and low-level topic feature, we combine them into a single doc for easy organization\n",
    "\n",
    "High_level_topic_path =  \"/\n",
    "Low_level_topic_path = \"/\n",
    "High_level_cluster_path = \"/\n",
    "Final_output_path = \"/\n",
    "topic_num = 25\n",
    "timepoints_num = 177\n",
    "\n",
    "for file in os.listdir(High_level_topic_path):\n",
    "    if file.startswith(\"DTM-\"):\n",
    "        output_dict = {}\n",
    "        for topic in range(0, topic_num):\n",
    "            output_dict[topic] = {}\n",
    "            for timepoint in range(0, timepoints_num):\n",
    "                output_dict[topic][timepoint] = []\n",
    "        keyword = file.split(\"-\")[2].split(\".\")[0]\n",
    "        #if keyword != \"work\":\n",
    "        #    print(\"Skip: \" + keyword)\n",
    "        #    continue\n",
    "        H_T = os.path.join(High_level_topic_path, file)\n",
    "        L_T = os.path.join(Low_level_topic_path, \"DTM-BERT-\" + keyword + \".csv\")\n",
    "        H_C = os.path.join(High_level_cluster_path, \"DTM_normal_\" + keyword + \".csv\")\n",
    "        with open(H_T, 'r') as csvfile: \n",
    "            #print(H_T)\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            header = next(csvreader) #topic_num, date_num, frequency_list, topic_vector_list\n",
    "            \n",
    "            for row in csvreader:\n",
    "                topic = int(row[0])\n",
    "                timepoint = int(row[1])\n",
    "                freq_list = json.loads(row[2])\n",
    "                topic_vector_list = json.loads(row[3])\n",
    "                output_dict[topic][timepoint].append(freq_list)\n",
    "                output_dict[topic][timepoint].append(topic_vector_list)\n",
    "        with open(L_T, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter = \",\", quotechar = '\"')\n",
    "            header = next(csvreader)\n",
    "            #print(header)#topic_num, date_num, keyword_string, keyword_freq, keyword_embedding\n",
    "            # Because L_T is separated into each keyword, we'll have to take this a bit differently\n",
    "            L_T_out = {}\n",
    "            for topic in range(0, topic_num):\n",
    "                L_T_out[topic] = {}\n",
    "                for timepoint in range(0, timepoints_num):\n",
    "                    L_T_out[topic][timepoint] = []\n",
    "            for row in csvreader:\n",
    "                topic = int(row[0])\n",
    "                timepoint = int(row[1])\n",
    "                keyword_string = row[2] #this is in fact not useful, we directly transform it into the number\n",
    "                keyword_id = dict_id[keyword_string]\n",
    "                keyword_freq = float(row[3])\n",
    "                #keyword_embedding = ast.literal_eval(row[4])\n",
    "                #L_T_out[topic][timepoint].append([keyword_string, keyword_freq, keyword_embedding])\n",
    "                L_T_out[topic][timepoint].append([keyword_id, keyword_freq]) # This is taking too much space, so we don't include keyword_embedding in the output\n",
    "            for topic in range(0, topic_num):\n",
    "                for timepoint in range(0, timepoints_num):\n",
    "                    output_dict[topic][timepoint].append(L_T_out[topic][timepoint])\n",
    "        with open(H_C, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter = \",\", quotechar = '\"')\n",
    "            header = next(csvreader)\n",
    "            #print(header)\n",
    "            label_list_H_C = header.copy()\n",
    "            for row in csvreader:\n",
    "                topic = int(row[0].split(\"_\")[2])\n",
    "                date = row[0].split(\"_\")[1] # in YY-MM-DD version\n",
    "                timepoint = int(calc_timedelta(datetime.datetime.strptime(date, '%Y-%m-%d')))\n",
    "                output_dict[topic][timepoint].extend(row[1:39])\n",
    "        #By this point the output_dict is finished.\n",
    "        outfile_name = keyword + \".csv\"\n",
    "        outpath = os.path.join(Final_output_path, outfile_name)\n",
    "        header_list = [\"topic_num\", \"date_num\", \"frequency_list\", \"topic_vector_list\", \"keywords_matrix\"]\n",
    "        header_list.extend(label_list_H_C[1:39])\n",
    "        with open(outpath, 'w') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(header_list)\n",
    "            for topic in range(0, topic_num):\n",
    "                for timepoint in range(0, timepoints_num):\n",
    "                    row = [topic, timepoint]\n",
    "                    row.extend(output_dict[topic][timepoint])\n",
    "                    csvwriter.writerow(row)\n",
    "        print(\"Finished: \" + keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d00f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_C = os.path.join(High_level_cluster_path, \"DTM_normal_\" + keyword + \".csv\")\n",
    "with open(H_C) as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter = \",\", quotechar = '\"')\n",
    "            header = next(csvreader)\n",
    "            print(header)\n",
    "            print(next(csvreader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list_H_C[1:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keyword_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('corona', return_tensors = \"pt\").to(device)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.pooler_output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "\n",
    "a.append(b)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_keyword_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('unique_keyword_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_keyword_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31305f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keyword_list = []\n",
    "for each in unique_keyword_dict:\n",
    "    unique_keyword_list.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_keyword_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_keyword_list, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id = {}\n",
    "for each in range(0, len(unique_keyword_list)):\n",
    "    dict_id[unique_keyword_list[each]] = each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id[\"corona\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_keyword_id.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_id, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bc303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a edge_list, this need a unique_keyword_list preconstructed\n",
    "\n",
    "for file in os.listdir(DTM_Model_Path):\n",
    "    if file.startswith(\"DTM-mdl-\"):\n",
    "        print(file)\n",
    "        keyword = file.split(\"-\")[2].split(\".\")[0]\n",
    "        output_list = []\n",
    "        mdl = tp.DTModel()\n",
    "        path = os.path.join(DTM_Model_Path, file)\n",
    "        mdl = mdl.load(file)\n",
    "        #Next we extract the features from the model.\n",
    "        for topic in range(0, topic_num):\n",
    "            for timepoint in range(0, timepoints_num):\n",
    "                keyword_id = []\n",
    "                input_text_dict = mdl.get_topic_words(topic_id = topic, timepoint = timepoint, top_n = 10) #We set it to get 10 keywords.\n",
    "                for each in input_text_dict:\n",
    "                    try:\n",
    "                        keyword_id.append(dict_id[each[0]])\n",
    "                    except Exception as e:\n",
    "                        keyword_id.append(0)\n",
    "                output = [topic, timepoint, keyword_id]\n",
    "                output_list.append(output)\n",
    "        name = \"DTM-KeywordID-\" + keyword + \".csv\"\n",
    "        filename3 = os.path.join(output_path, name)\n",
    "        with open(filename3, 'w') as csvfile: \n",
    "            # creating a csv writer object \n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerows(output_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
