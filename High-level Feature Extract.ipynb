{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53344b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automated Readability Index (ARI)\n",
    "#Calculation:\n",
    "# 4.71 (Character / Word) + 0.5(Word / Sentence) - 21.43\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#number_of_sentences = sent_tokenize(sentences)\n",
    "#countOfWords = len(\"Geeksforgeeks is best Computer Science Portal\".split())\n",
    "#print(\"Count of Words in the given Sentence:\", countOfWords)\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import datetime\n",
    "def ARI(paragraph):\n",
    "    countofwords = len(paragraph.split())\n",
    "    numofsentences = len(sent_tokenize(paragraph))\n",
    "    if countofwords == 0:\n",
    "        countofwords = 1\n",
    "    if numofsentences == 0:\n",
    "        numofsentences = 1\n",
    "    ari = 4.71 * len(paragraph) / countofwords + 0.5 * countofwords / numofsentences -21.43\n",
    "    return ari\n",
    "def converttimestamp(timestamp_string):\n",
    "    obj = datetime.datetime.strptime(timestamp_string, '%a %b %d %H:%M:%S %z %Y')\n",
    "    return obj\n",
    "\n",
    "def calc_timedelta(ts1, ts2):\n",
    "    ts1_obj = converttimestamp(ts1)\n",
    "    ts2_obj = converttimestamp(ts2)\n",
    "    return (ts1_obj - ts2_obj).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High level feature extractor. \n",
    "# We originally split these features into two types, ARI (text features) and UGP (user profile features)\n",
    "import os\n",
    "import json\n",
    "import signal\n",
    "import math\n",
    "import preprocessor as p\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "import fasttext\n",
    "import pickle\n",
    "import numpy as np\n",
    "model = fasttext.load_model()\n",
    "file = open('', 'rb')\n",
    "svm = pickle.load(file)\n",
    "def get_doc_vector_fasttext(document, model):\n",
    "    vector_document = [model[i] for i in document.split() if i in model.words]\n",
    "    document_to_vector = np.sum(vector_document, axis=0)\n",
    "    return document_to_vector\n",
    "#import preprocessing as p\n",
    "def get_prediction_misinfo(document, model_fasttext, model_svm):\n",
    "    vector_document = [model_fasttext[i] for i in document.split() if i in model_fasttext.words]\n",
    "    try:\n",
    "        document_to_vector = np.sum(vector_document, axis=0)  \n",
    "        return svm.predict([document_to_vector])[0]\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "def cal_avg_ARI_all_split(path, keyword):\n",
    "    final_result = {}\n",
    "    in_path = os.path.join(path,keyword)\n",
    "    for each in os.listdir(in_path):\n",
    "        result = cal_avg_ARI_day_split(os.path.join(in_path, each), 4)\n",
    "        for each_result in result:\n",
    "            final_string = each + \"_\" + each_result\n",
    "            re = [result[each_result][\"total_ARI\"], result[each_result][\"total_pos\"], result[each_result][\"total_neg\"], result[each_result][\"total_neu\"], \n",
    "                  result[each_result][\"total_comp\"],result[each_result][\"total_subj\"], result[each_result][\"total_pol\"], result[each_result][\"ARI_sd\"],\n",
    "                  result[each_result][\"pos_sd\"], result[each_result][\"neg_sd\"], result[each_result][\"neu_sd\"], result[each_result][\"comp_sd\"], \n",
    "                  result[each_result][\"subj_sd\"], result[each_result][\"pol_sd\"], result[each_result][\"avg_part\"], result[each_result][\"part_sd\"],\n",
    "                  result[each_result][\"avg_misinfo\"], result[each_result][\"total_para\"]]\n",
    "            final_result[final_string] = re\n",
    "    #Write to file\n",
    "    __PATH__ = \"\"\n",
    "    \n",
    "    json_object = json.dumps(final_result, indent = 4) \n",
    "    with open(os.path.join(__PATH__, keyword) + \".json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    outfile.close()    \n",
    "def cal_avg_UGP_all_split(path, keyword):\n",
    "    final_result = {}\n",
    "    in_path = os.path.join(path,keyword)\n",
    "    for each in os.listdir(in_path):\n",
    "        result = cal_avg_UGP_day_split(os.path.join(in_path, each), 4)\n",
    "        for each_result in result:\n",
    "            final_string = each + \"_\" + each_result\n",
    "            re = [result[each_result][\"total_verified\"], result[each_result][\"total_geo_enabled\"], result[each_result][\"total_fol_count\"], \n",
    "                  result[each_result][\"total_fri_count\"], result[each_result][\"total_stat_count\"], result[each_result][\"total_fav_count\"],\n",
    "                  result[each_result][\"total_list_created\"],result[each_result][\"total_account_age\"], result[each_result][\"total_UGP_word_count\"], \n",
    "                  result[each_result][\"total_screen_name_length\"],result[each_result][\"total_UGP_ARI\"], result[each_result][\"fol_count_sd\"],\n",
    "                  result[each_result][\"fri_count_sd\"], result[each_result][\"stat_count_sd\"],result[each_result][\"fav_count_sd\"], \n",
    "                  result[each_result][\"list_created_sd\"], result[each_result][\"account_age_sd\"], result[each_result][\"word_count_sd\"],\n",
    "                  result[each_result][\"name_length_sd\"], result[each_result][\"UGP_ARI_sd\"], result[each_result][\"total_para\"]]\n",
    "            final_result[final_string] = re\n",
    "    #Write to file\n",
    "    __PATH__ = \"/home/ps/luyue/UGP_RESULT\"\n",
    "    \n",
    "    json_object = json.dumps(final_result, indent = 4) \n",
    "    with open(os.path.join(__PATH__, keyword) + \".json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    outfile.close()\n",
    "def cal_avg_ARI_day_split(day_path, split_num = 4):\n",
    "    #Split num must be dividable by 24.\n",
    "    extract_data = {}\n",
    "    for i in range(0, split_num):\n",
    "        string = \"slot_\" + str(i)\n",
    "        extract_data[string] = {\"total_ARI\": 0,\"total_pos\": 0, \"total_neg\": 0, \"total_neu\" : 0, \"total_comp\" : 0, \n",
    "            \"total_subj\": 0, \"total_pol\": 0, \"ARI_sd\" : 0, \"pos_sd\" : 0, \"neg_sd\" : 0, \"total_misinfo\": 0,\n",
    "            \"neu_sd\": 0, \"comp_sd\": 0, \"subj_sd\": 0, \"pol_sd\": 0, \"avg_part\": 0, \n",
    "            \"part_sd\": 0, \"avg_misinfo\": 0, \"total_para\": 0, \"ARI_list\" : [], \"pos_list\": [], \"neg_list\": [], \"neu_list\": [], \"comp_list\": [],\n",
    "            \"subj_list\": [], \"pol_list\": [], \"message_list\": {}, \"participation_count_list\": []}\n",
    "    for each in os.listdir(day_path):\n",
    "        with open(os.path.join(day_path, each), 'r', encoding = 'utf-8') as f:\n",
    "            try:\n",
    "                listdict = splitdict(f.read())\n",
    "            except Exception as e:\n",
    "                listdict = []\n",
    "            signal.alarm(180)\n",
    "            try:\n",
    "                for each in listdict:\n",
    "                    timestamp = converttimestamp(each['created_at'])\n",
    "                    slot = math.floor(timestamp.hour / 6)\n",
    "                    slot_string = \"slot_\" + str(slot)\n",
    "                    if 'retweeted_status' in each.keys():\n",
    "                        st = p.clean(each['retweeted_status'][\"full_text\"].lower())\n",
    "                    else:\n",
    "                        st = p.clean(each[\"full_text\"].lower())\n",
    "                    st2 = p.clean(each['user'][\"description\"].lower())    \n",
    "                    blob = TextBlob(st)\n",
    "                    x = sia.polarity_scores(st)\n",
    "                    extract_data[slot_string][\"total_pos\"] += x[\"pos\"]\n",
    "                    extract_data[slot_string][\"total_neg\"] += x[\"neg\"]\n",
    "                    extract_data[slot_string][\"total_neu\"] += x[\"neu\"]\n",
    "                    extract_data[slot_string][\"total_comp\"] += x[\"compound\"]\n",
    "                    extract_data[slot_string][\"total_subj\"] += blob.sentiment.subjectivity\n",
    "                    extract_data[slot_string][\"total_pol\"] += blob.sentiment.polarity\n",
    "                    extract_data[slot_string][\"total_misinfo\"] += get_prediction_misinfo(st, model, svm)\n",
    "                    x2 = ARI(st)\n",
    "                    extract_data[slot_string][\"total_ARI\"] += x2\n",
    "                    extract_data[slot_string][\"total_para\"] += 1\n",
    "                    extract_data[slot_string][\"ARI_list\"].append(x2)\n",
    "                    extract_data[slot_string][\"pos_list\"].append(x[\"pos\"])\n",
    "                    extract_data[slot_string][\"neg_list\"].append(x[\"neg\"])\n",
    "                    extract_data[slot_string][\"neu_list\"].append(x[\"neu\"])\n",
    "                    extract_data[slot_string][\"comp_list\"].append(x[\"compound\"])\n",
    "                    extract_data[slot_string][\"subj_list\"].append(blob.sentiment.subjectivity)\n",
    "                    extract_data[slot_string][\"pol_list\"].append(blob.sentiment.polarity)\n",
    "                    if st in extract_data[slot_string][\"message_list\"]:\n",
    "                        extract_data[slot_string][\"message_list\"][st] += 1\n",
    "                    else:\n",
    "                        extract_data[slot_string][\"message_list\"][st] = 1         \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            signal.alarm(0)\n",
    "            f.close()\n",
    "    for slot_string in extract_data:    \n",
    "        for x in extract_data[slot_string][\"message_list\"]:\n",
    "            extract_data[slot_string][\"participation_count_list\"].append(extract_data[slot_string][\"message_list\"][x]) \n",
    "        extract_data[slot_string][\"avg_part\"] = extract_data[slot_string][\"total_para\"] / (len(extract_data[slot_string][\"participation_count_list\"]) + 1)\n",
    "        extract_data[slot_string][\"avg_misinfo\"] = extract_data[slot_string][\"total_misinfo\"] / (extract_data[slot_string][\"total_para\"] + 1)\n",
    "        extract_data[slot_string][\"part_sd\"] = np.std(extract_data[slot_string][\"participation_count_list\"])\n",
    "        extract_data[slot_string][\"ARI_sd\"] = np.std(extract_data[slot_string][\"ARI_list\"])\n",
    "        extract_data[slot_string][\"pos_sd\"] = np.std(extract_data[slot_string][\"pos_list\"])\n",
    "        extract_data[slot_string][\"neg_sd\"] = np.std(extract_data[slot_string][\"neg_list\"])\n",
    "        extract_data[slot_string][\"neu_sd\"] = np.std(extract_data[slot_string][\"neu_list\"])\n",
    "        extract_data[slot_string][\"comp_sd\"] = np.std(extract_data[slot_string][\"comp_list\"])\n",
    "        extract_data[slot_string][\"subj_sd\"] = np.std(extract_data[slot_string][\"subj_list\"])\n",
    "        extract_data[slot_string][\"pol_sd\"] = np.std(extract_data[slot_string][\"pol_list\"])\n",
    "    return extract_data\n",
    "def cal_avg_UGP_day_split(day_path, split_num = 4):\n",
    "    extract_data = {}\n",
    "    for i in range(0, split_num):\n",
    "        string = \"slot_\" + str(i)\n",
    "        extract_data[string] = {\"total_verified\": 0, \"total_geo_enabled\": 0, \"total_fol_count\": 0, \"total_fri_count\": 0, \"total_stat_count\": 0, \n",
    "           \"total_fav_count\": 0, \"total_list_created\": 0,\"total_account_age\": 0, \"total_UGP_word_count\": 0, \"total_screen_name_length\": 0,\n",
    "           \"total_UGP_ARI\": 0, \"fol_count_sd\": 0, \"fri_count_sd\": 0, \"stat_count_sd\": 0,\"fav_count_sd\": 0, \n",
    "           \"list_created_sd\": 0, \"account_age_sd\": 0, \"word_count_sd\": 0, \"name_length_sd\": 0, \"UGP_ARI_sd\": 0, \n",
    "           \"total_para\": 0, \"UGP_ARI_list\": [], \"name_length_list\": [], \"word_count_list\": [], \"account_age_list\": [], \"list_created_list\": [],\n",
    "           \"fav_count_list\": [], \"stat_count_list\": [], \"fri_count_list\": [], \"fol_count_list\": []}\n",
    "    for each in os.listdir(day_path):\n",
    "        with open(os.path.join(day_path, each), 'r', encoding = 'utf-8') as f:\n",
    "            try:\n",
    "                listdict = splitdict(f.read())\n",
    "            except Exception as e:\n",
    "                listdict = []\n",
    "            signal.alarm(180)\n",
    "            try:\n",
    "                for each in listdict:\n",
    "                    timestamp = converttimestamp(each['created_at'])\n",
    "                    slot = math.floor(timestamp.hour / 6)\n",
    "                    slot_string = \"slot_\" + str(slot)                    \n",
    "                    st = p.clean(each['user'][\"description\"].lower())\n",
    "                    UGP_ARI = ARI(st)\n",
    "                    #blob = TextBlob(st)\n",
    "                    screen_name_length = len(each['user']['screen_name'])\n",
    "                    UGP_word_count = len(each['user']['description'].split())\n",
    "                    account_age = calc_timedelta(each['created_at'], each['user']['created_at'])\n",
    "                    list_created = each['user']['listed_count']\n",
    "                    fav_count = each['user']['favourites_count']\n",
    "                    stat_count = each['user']['statuses_count']\n",
    "                    fri_count = each['user']['friends_count']\n",
    "                    fol_count = each['user']['followers_count']\n",
    "                    if each['user']['geo_enabled']:\n",
    "                        geo_enable = 1\n",
    "                    else:\n",
    "                        geo_enable = 0\n",
    "                    if each['user']['verified']:\n",
    "                        verified = 1\n",
    "                    else:\n",
    "                        verified = 0\n",
    "                    extract_data[slot_string][\"total_UGP_ARI\"] += UGP_ARI\n",
    "                    extract_data[slot_string][\"total_verified\"] += verified\n",
    "                    extract_data[slot_string][\"total_geo_enabled\"] += geo_enable\n",
    "                    extract_data[slot_string][\"total_fol_count\"] += fol_count\n",
    "                    extract_data[slot_string][\"total_fri_count\"] += fri_count\n",
    "                    extract_data[slot_string][\"total_stat_count\"] += stat_count\n",
    "                    extract_data[slot_string][\"total_fav_count\"] += fav_count\n",
    "                    extract_data[slot_string][\"total_list_created\"] += list_created\n",
    "                    extract_data[slot_string][\"total_account_age\"] += account_age\n",
    "                    extract_data[slot_string][\"total_UGP_word_count\"] += UGP_word_count\n",
    "                    extract_data[slot_string][\"total_screen_name_length\"] += screen_name_length\n",
    "                    extract_data[slot_string][\"total_para\"] += 1\n",
    "                    extract_data[slot_string][\"UGP_ARI_list\"].append(UGP_ARI)\n",
    "                    extract_data[slot_string][\"name_length_list\"].append(screen_name_length)\n",
    "                    extract_data[slot_string][\"word_count_list\"].append(UGP_word_count)\n",
    "                    extract_data[slot_string][\"account_age_list\"].append(account_age)\n",
    "                    extract_data[slot_string][\"list_created_list\"].append(list_created)\n",
    "                    extract_data[slot_string][\"fav_count_list\"].append(fav_count)\n",
    "                    extract_data[slot_string][\"stat_count_list\"].append(stat_count)\n",
    "                    extract_data[slot_string][\"fri_count_list\"].append(fri_count)\n",
    "                    extract_data[slot_string][\"fol_count_list\"].append(fol_count)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            signal.alarm(0)\n",
    "            f.close()\n",
    "        extract_data[slot_string][\"UGP_ARI_sd\"] = np.std(extract_data[slot_string][\"UGP_ARI_list\"])\n",
    "        extract_data[slot_string][\"name_length_sd\"] = np.std(extract_data[slot_string][\"name_length_list\"])\n",
    "        extract_data[slot_string][\"word_count_sd\"] = np.std(extract_data[slot_string][\"word_count_list\"])\n",
    "        extract_data[slot_string][\"account_age_sd\"] = np.std(extract_data[slot_string][\"account_age_list\"])\n",
    "        extract_data[slot_string][\"list_created_sd\"] = np.std(extract_data[slot_string][\"list_created_list\"])\n",
    "        extract_data[slot_string][\"fav_count_sd\"] = np.std(extract_data[slot_string][\"fav_count_list\"])\n",
    "        extract_data[slot_string][\"stat_count_sd\"] = np.std(extract_data[slot_string][\"stat_count_list\"])\n",
    "        extract_data[slot_string][\"fri_count_sd\"] = np.std(extract_data[slot_string][\"fri_count_list\"])\n",
    "        extract_data[slot_string][\"fol_count_sd\"] = np.std(extract_data[slot_string][\"fol_count_list\"])\n",
    "    #print(\"Finished:\" + day_path)\n",
    "    return extract_data\n",
    "def cal_avg_ARI_day(day_path):\n",
    "    total_ARI = 0\n",
    "    total_para = 0\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    total_neu = 0\n",
    "    total_comp = 0\n",
    "    total_subj = 0\n",
    "    total_pol = 0\n",
    "    total_misinfo = 0\n",
    "    ARI_list = []\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    neu_list = []\n",
    "    comp_list = []\n",
    "    message_list = {}\n",
    "    participation_count_list = []\n",
    "    subj_list = []\n",
    "    pol_list = []\n",
    "    for each in os.listdir(day_path):\n",
    "        with open(os.path.join(day_path, each), 'r', encoding = 'utf-8') as f:\n",
    "            try:\n",
    "                listdict = splitdict(f.read())\n",
    "            except Exception as e:\n",
    "                listdict = []\n",
    "            signal.alarm(180)\n",
    "            try:\n",
    "                for each in listdict:\n",
    "                    if 'retweeted_status' in each.keys():\n",
    "                        st = p.clean(each['retweeted_status'][\"full_text\"].lower())\n",
    "                    else:\n",
    "                        st = p.clean(each[\"full_text\"].lower())\n",
    "                    st2 = p.clean(each['user'][\"description\"].lower())\n",
    "                    blob = TextBlob(st)\n",
    "                    x = sia.polarity_scores(st)\n",
    "                    total_pos += x[\"pos\"]\n",
    "                    total_neg += x[\"neg\"]\n",
    "                    total_neu += x[\"neu\"]\n",
    "                    total_comp += x[\"compound\"]\n",
    "                    total_subj += blob.sentiment.subjectivity\n",
    "                    total_pol += blob.sentiment.polarity\n",
    "                    total_misinfo += get_prediction_misinfo(st, model, svm)\n",
    "                    x2 = ARI(st)\n",
    "                    total_ARI += x2\n",
    "                    total_para += 1\n",
    "                    ARI_list.append(x2)\n",
    "                    pos_list.append(x[\"pos\"])\n",
    "                    neg_list.append(x[\"neg\"])\n",
    "                    neu_list.append(x[\"neu\"])\n",
    "                    comp_list.append(x[\"compound\"])\n",
    "                    subj_list.append(blob.sentiment.subjectivity)\n",
    "                    pol_list.append(blob.sentiment.polarity)\n",
    "                    if st in message_list:\n",
    "                        message_list[st] += 1\n",
    "                    else:\n",
    "                        message_list[st] = 1\n",
    "                    #message_list[st] = 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            signal.alarm(0)\n",
    "            f.close()\n",
    "    for x in message_list:\n",
    "        participation_count_list.append(message_list[x])\n",
    "    avg_part = total_para / (len(participation_count_list) + 1)\n",
    "    avg_misinfo = total_misinfo / (total_para + 1)\n",
    "    part_sd = np.std(participation_count_list)\n",
    "    ARI_sd = np.std(ARI_list)\n",
    "    pos_sd = np.std(pos_list)\n",
    "    neg_sd = np.std(neg_list)\n",
    "    neu_sd = np.std(neu_list)\n",
    "    comp_sd = np.std(comp_list)\n",
    "    subj_sd = np.std(subj_list)\n",
    "    pol_sd = np.std(pol_list)\n",
    "    return [total_ARI,total_pos, total_neg, total_neu, total_comp, \n",
    "            total_subj, total_pol, ARI_sd, pos_sd, neg_sd, \n",
    "            neu_sd, comp_sd, subj_sd, pol_sd, avg_part, \n",
    "            part_sd,avg_misinfo, total_para]\n",
    "def cal_avg_UGP_day(day_path):\n",
    "    total_verified = 0\n",
    "    total_geo_enabled = 0\n",
    "    total_fol_count = 0\n",
    "    total_fri_count = 0\n",
    "    total_stat_count = 0\n",
    "    total_fav_count = 0\n",
    "    total_list_created = 0\n",
    "    total_account_age = 0\n",
    "    total_UGP_word_count = 0\n",
    "    total_screen_name_length = 0\n",
    "    total_UGP_ARI = 0\n",
    "    total_para = 0\n",
    "    UGP_ARI_list = []\n",
    "    name_length_list = []\n",
    "    word_count_list = []\n",
    "    account_age_list = []\n",
    "    list_created_list = []\n",
    "    fav_count_list = []\n",
    "    stat_count_list = []\n",
    "    fri_count_list = []\n",
    "    fol_count_list = []\n",
    "    for each in os.listdir(day_path):\n",
    "        with open(os.path.join(day_path, each), 'r', encoding = 'utf-8') as f:\n",
    "            try:\n",
    "                listdict = splitdict(f.read())\n",
    "            except Exception as e:\n",
    "                listdict = []\n",
    "            signal.alarm(180)\n",
    "            try:\n",
    "                for each in listdict:\n",
    "                    st = p.clean(each['user'][\"description\"].lower())\n",
    "                    UGP_ARI = ARI(st)\n",
    "                    #blob = TextBlob(st)\n",
    "                    screen_name_length = len(each['user']['screen_name'])\n",
    "                    UGP_word_count = len(each['user']['description'].split())\n",
    "                    account_age = calc_timedelta(each['created_at'], each['user']['created_at'])\n",
    "                    list_created = each['user']['listed_count']\n",
    "                    fav_count = each['user']['favourites_count']\n",
    "                    stat_count = each['user']['statuses_count']\n",
    "                    fri_count = each['user']['friends_count']\n",
    "                    fol_count = each['user']['followers_count']\n",
    "                    if each['user']['geo_enabled']:\n",
    "                        geo_enable = 1\n",
    "                    else:\n",
    "                        geo_enable = 0\n",
    "                    if each['user']['verified']:\n",
    "                        verified = 1\n",
    "                    else:\n",
    "                        verified = 0\n",
    "                    total_UGP_ARI += UGP_ARI\n",
    "                    total_verified += verified\n",
    "                    total_geo_enabled += geo_enable\n",
    "                    total_fol_count += fol_count\n",
    "                    total_fri_count += fri_count\n",
    "                    total_stat_count += stat_count\n",
    "                    total_fav_count += fav_count\n",
    "                    total_list_created += list_created\n",
    "                    total_account_age += account_age\n",
    "                    total_UGP_word_count += UGP_word_count\n",
    "                    total_screen_name_length += screen_name_length\n",
    "                    total_para += 1\n",
    "                    UGP_ARI_list.append(UGP_ARI)\n",
    "                    name_length_list.append(screen_name_length)\n",
    "                    word_count_list.append(UGP_word_count)\n",
    "                    account_age_list.append(account_age)\n",
    "                    list_created_list.append(list_created)\n",
    "                    fav_count_list.append(fav_count)\n",
    "                    stat_count_list.append(stat_count)\n",
    "                    fri_count_list.append(fri_count)\n",
    "                    fol_count_list.append(fol_count)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            signal.alarm(0)\n",
    "            f.close()\n",
    "    UGP_ARI_sd = np.std(UGP_ARI_list)\n",
    "    name_length_sd = np.std(name_length_list)\n",
    "    word_count_sd = np.std(word_count_list)\n",
    "    account_age_sd = np.std(account_age_list)\n",
    "    list_created_sd = np.std(list_created_list)\n",
    "    fav_count_sd = np.std(fav_count_list)\n",
    "    stat_count_sd = np.std(stat_count_list)\n",
    "    fri_count_sd = np.std(fri_count_list)\n",
    "    fol_count_sd = np.std(fol_count_list)\n",
    "    #print(\"Finished:\" + day_path)\n",
    "    return[total_verified, total_geo_enabled, total_fol_count, total_fri_count, total_stat_count, \n",
    "           total_fav_count, total_list_created,total_account_age, total_UGP_word_count, total_screen_name_length,\n",
    "           total_UGP_ARI, fol_count_sd, fri_count_sd, stat_count_sd,fav_count_sd, \n",
    "           list_created_sd, account_age_sd, word_count_sd, name_length_sd, UGP_ARI_sd, \n",
    "           total_para]\n",
    "def cal_avg_ARI_all(path, keyword):\n",
    "    result = {}\n",
    "    in_path = os.path.join(path,keyword)\n",
    "    for each in os.listdir(in_path):\n",
    "        result[each] = cal_avg_ARI_day(os.path.join(in_path, each))\n",
    "    #Write to file\n",
    "    __PATH__ = \"/home/ps/luyue/ARI_RESULT_DTM\"\n",
    "    \n",
    "    json_object = json.dumps(result, indent = 4) \n",
    "    with open(os.path.join(__PATH__, keyword) + \".json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    outfile.close()\n",
    "def cal_avg_ARI_DTM(main_path, keyword):\n",
    "    __PATH__ = \"/home/ps/luyue/ARI_RESULT_DTM\"    \n",
    "    result = {}\n",
    "    keyword_path = os.path.join(main_path, keyword)\n",
    "    for each_day in os.listdir(keyword_path):\n",
    "        day_path = os.path.join(keyword_path, each_day)\n",
    "        for each_topic in os.listdir(day_path):\n",
    "            topic_path = os.path.join(day_path, each_topic)\n",
    "            result_string = keyword + \"_\" + str(each_day) + \"_\" + str(each_topic)\n",
    "            result[result_string] = cal_avg_ARI_day(topic_path)\n",
    "    json_object = json.dumps(result, indent = 4)\n",
    "    with open(os.path.join(__PATH__, keyword) + \".json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    outfile.close()\n",
    "def cal_avg_UGP_DTM(main_path, keyword):\n",
    "    __PATH__ = \"/home/ps/luyue/UGP_RESULT_DTM\"    \n",
    "    result = {}\n",
    "    keyword_path = os.path.join(main_path, keyword)\n",
    "    for each_day in os.listdir(keyword_path):\n",
    "        day_path = os.path.join(keyword_path, each_day)\n",
    "        for each_topic in os.listdir(day_path):\n",
    "            topic_path = os.path.join(day_path, each_topic)\n",
    "            result_string = keyword + \"_\" + str(each_day) + \"_\" + str(each_topic)\n",
    "            result[result_string] = cal_avg_UGP_day(topic_path)\n",
    "    json_object = json.dumps(result, indent = 4)\n",
    "    with open(os.path.join(__PATH__, keyword) + \".json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    outfile.close()\n",
    "def cal_avg_UGP_all(path, keyword):\n",
    "    result = {}\n",
    "    in_path = os.path.join(path,keyword)\n",
    "    for each in os.listdir(in_path):\n",
    "        result[each] = cal_avg_UGP_day(os.path.join(in_path, each))\n",
    "    #Write to file\n",
    "    __PATH__ = \"/home/ps/luyue/UGP_RESULT_DTM\"\n",
    "    \n",
    "    json_object = json.dumps(result, indent = 4) \n",
    "    with open(os.path.join(__PATH__, keyword) + \".json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    outfile.close()\n",
    "def cal_avg_ARI_keyword(path, keyword):\n",
    "    result = {}\n",
    "    in_path = os.path.join(path, keyword)\n",
    "    for each_day in os.listdir(in_path):\n",
    "        day_path = os.path.join(in_path, day_path)\n",
    "        result[each_day] = {}\n",
    "        for each_topic in os.listdir(day_path):\n",
    "            result[each_day][each_topic] = cal_avg_ARI_day(os.path.join(day_path, each_topic))\n",
    "def splitdict(string):\n",
    "    i = 0\n",
    "    dict_list = []\n",
    "    for index in range(len(string) - 2):\n",
    "        if string[index] == \"}\" and string[index+1] == \"{\" and string[index+2].isalpha() == False:\n",
    "            sub_string = string[i:index+1]\n",
    "            i = index + 1\n",
    "            try:\n",
    "                dict1 = json.loads(sub_string)\n",
    "                dict_list.append(dict1)\n",
    "            except Exception as e:\n",
    "                dict1 = []\n",
    "                #print(sub_string)\n",
    "                print(\"ERROR!\")\n",
    "                print(e)\n",
    "                pass\n",
    "    sub_string = string[i:]\n",
    "    dict2 = json.loads(sub_string)\n",
    "    dict_list.append(dict2)\n",
    "    return dict_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72282b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proc_list = []\n",
    "dir1 = \"*\" #Directory for your separated topic posts\n",
    "import multiprocessing\n",
    "proc_list = []\n",
    "for each in os.listdir(dir1):\n",
    "    proc = multiprocessing.Process(target = cal_avg_ARI_DTM, args = (dir1, each, ))\n",
    "    proc_list.append(proc)\n",
    "    proc.start()\n",
    "for process in proc_list:\n",
    "    process.join()\n",
    "proc_list = []\n",
    "for each in os.listdir(dir1):\n",
    "    proc = multiprocessing.Process(target = cal_avg_UGP_DTM, args = (dir1, each, ))\n",
    "    proc_list.append(proc)\n",
    "    proc.start()\n",
    "for process in proc_list:\n",
    "    process.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
